@inproceedings{duRecurrentMarkedTemporal2016,
  title = {Recurrent {{Marked Temporal Point Processes}}: {{Embedding Event History}} to {{Vector}}},
  shorttitle = {Recurrent {{Marked Temporal Point Processes}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Du, Nan and Dai, Hanjun and Trivedi, Rakshit and Upadhyay, Utkarsh and Gomez-Rodriguez, Manuel and Song, Le},
  date = {2016-08-13},
  pages = {1555--1564},
  publisher = {{ACM}},
  location = {{San Francisco California USA}},
  doi = {10.1145/2939672.2939875},
  abstract = {Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.},
  eventtitle = {{{KDD}} '16: {{The}} 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-4232-2},
  langid = {english},
  annotation = {427 citations (Semantic Scholar/DOI) [2022-05-26]},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\M2DIYM8G\\Du et al. - 2016 - Recurrent Marked Temporal Point Processes Embeddi.pdf}
}

@inproceedings{enguehardNeuralTemporalPoint2020,
  title = {Neural {{Temporal Point Processes For Modelling Electronic Health Records}}},
  booktitle = {Proceedings of the {{Machine Learning}} for {{Health NeurIPS Workshop}}},
  author = {Enguehard, Joseph and Busbridge, Dan and Bozson, Adam and Woodcock, Claire and Hammerla, Nils},
  date = {2020-11-23},
  pages = {85--113},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v136/enguehard20a.html},
  urldate = {2022-05-06},
  abstract = {The modelling of Electronic Health Records (EHRs) has the potential to drive more efficient allocation of healthcare resources, enabling early intervention strategies and advancing personalised healthcare. However, EHRs are challenging to model due to their realisation as noisy, multi-modal data occurring at irregular time intervals. To address their temporal nature, we treat EHRs as samples generated by a Temporal Point Process (TPP), enabling us to model what happened in an event with when it happened in a principled way. We gather and propose neural network parameterisations of TPPs, collectively referred to as Neural TPPs. We perform evaluations on synthetic EHRs as well as on a set of established benchmarks. We show that TPPs significantly outperform their non-TPP counterparts on EHRs. We also show that an assumption of many Neural TPPs, that the class distribution is conditionally independent of time, reduces performance on EHRs. Finally, our proposed attention-based Neural TPP performs favourably compared to existing models, whilst aligning with real world interpretability requirements, an important step towards a component of clinical decision support systems.},
  eventtitle = {Machine {{Learning}} for {{Health}}},
  langid = {english},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\L89R3ZXZ\\Enguehard et al_2020_Neural Temporal Point Processes For Modelling Electronic Health Records.pdf}
}

@inproceedings{hornSetFunctionsTime2020,
  title = {Set {{Functions}} for {{Time Series}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Horn, Max and Moor, Michael and Bock, Christian and Rieck, Bastian and Borgwardt, Karsten},
  date = {2020-11-21},
  pages = {4353--4363},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/horn20a.html},
  urldate = {2022-09-26},
  abstract = {Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that commonly occur in real-world datasets, especially in healthcare applications. This paper proposes a novel approach for classifying irregularly-sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Our method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable with a beneficial memory footprint, thus scaling well to large datasets of long time series and online monitoring scenarios. Furthermore, our approach permits quantifying per-observation contributions to the classification outcome. We extensively compare our method with existing algorithms on multiple healthcare time series datasets and demonstrate that it performs competitively whilst significantly reducing runtime.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\ETCDFTT8\\Horn et al. - 2020 - Set Functions for Time Series.pdf;C\:\\Users\\hokarami\\Zotero\\storage\\V6B5HGHU\\Horn et al_2020_Set Functions for Time Series.pdf}
}

@inproceedings{meiNeuralHawkesProcess2017,
  title = {The {{Neural Hawkes Process}}: {{A Neurally Self-Modulating Multivariate Point Process}}},
  shorttitle = {The {{Neural Hawkes Process}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mei, Hongyuan and Eisner, Jason M},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/6463c88460bd63bbe256e495c63aa40b-Abstract.html},
  urldate = {2022-05-26},
  abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited—in the sense of having their probabilities elevated or decreased—by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\NR4DXBI3\\supp.pdf;C\:\\Users\\hokarami\\Zotero\\storage\\S8LIWHF4\\Mei_Eisner_2017_The Neural Hawkes Process.pdf}
}

@inproceedings{salehiLearningHawkesProcesses2019,
  title = {Learning {{Hawkes Processes}} from a Handful of Events},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Salehi, Farnood and Trouleau, William and Grossglauser, Matthias and Thiran, Patrick},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/8767bccb1ff4231a9962e3914f4f1f8f-Abstract.html},
  urldate = {2022-05-29},
  abstract = {Learning the causal-interaction network of multivariate Hawkes processes is a useful task in many applications. Maximum-likelihood estimation is the most common approach to solve the problem in the presence of long observation sequences. However, when only short sequences are available, the lack of data amplifies the risk of overfitting and regularization becomes critical. Due to the challenges of hyper-parameter tuning, state-of-the-art methods only parameterize regularizers by a single shared hyper-parameter, hence limiting the power of representation of the model. To solve both issues, we develop in this work an efficient algorithm based on variational expectation-maximization. Our approach is able to optimize over an extended set of hyper-parameters. It is also able to take into account the uncertainty in the model parameters by learning a posterior distribution over them. Experimental results on both synthetic and real datasets show that our approach significantly outperforms state-of-the-art methods under short observation sequences.},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\DJG8NDM6\\Salehi et al_2019_Learning Hawkes Processes from a handful of events.pdf}
}

@inproceedings{zhangCAUSELearningGranger2020,
  title = {{{CAUSE}}: {{Learning Granger Causality}} from {{Event Sequences}} Using {{Attribution Methods}}},
  shorttitle = {{{CAUSE}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Wei and Panum, Thomas and Jha, Somesh and Chalasani, Prasad and Page, David},
  date = {2020-11-21},
  pages = {11235--11245},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/zhang20v.html},
  urldate = {2022-05-08},
  abstract = {We study the problem of learning Granger causality between event types from asynchronous, interdependent, multi-type event sequences. Existing work suffers from either limited model flexibility or poor model explainability and thus fails to uncover Granger causality across a wide variety of event sequences with diverse event interdependency. To address these weaknesses, we propose CAUSE (Causality from AttribUtions on Sequence of Events), a novel framework for the studied task. The key idea of CAUSE is to first implicitly capture the underlying event interdependency by fitting a neural point process, and then extract from the process a Granger causality statistic using an axiomatic attribution method. Across multiple datasets riddled with diverse event interdependency, we demonstrate that CAUSE achieves superior performance on correctly inferring the inter-type Granger causality over a range of state-of-the-art methods.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\CZMH8EWN\\Zhang et al_2020_CAUSE.pdf;C\:\\Users\\hokarami\\Zotero\\storage\\DGGCPN84\\Zhang et al. - 2020 - CAUSE Learning Granger Causality from Event Seque.pdf}
}

@inproceedings{zhangLearningNeuralPoint2021,
  title = {Learning {{Neural Point Processes}} with {{Latent Graphs}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Zhang, Qiang and Lipani, Aldo and Yilmaz, Emine},
  date = {2021-04-19},
  pages = {1495--1505},
  publisher = {{ACM}},
  location = {{Ljubljana Slovenia}},
  doi = {10.1145/3442381.3450135},
  abstract = {Neural point processes (NPPs) employ neural networks to capture complicated dynamics of asynchronous event sequences. Existing NPPs feed all history events into neural networks, assuming that all event types contribute to the prediction of the target type. However, this assumption can be problematic because in reality some event types do not contribute to the predictions of another type. To correct this defect, we learn to omit those types of events that do not contribute to the prediction of one target type during the formulation of NPPs. Towards this end, we simultaneously consider the tasks of (1) finding event types that contribute to predictions of the target types and (2) learning a NPP model from event sequences. For the former, we formulate a latent graph, with event types being vertices and non-zero contributing relationships being directed edges; then we propose a probabilistic graph generator, from which we sample a latent graph. For the latter, the sampled graph can be readily used as a plug-in to modify an existing NPP model. Because these two tasks are nested, we propose to optimize the model parameters through bilevel programming, and develop an efficient solution based on truncated gradient back-propagation. Experimental results on both synthetic and real-world datasets show the improved performance against state-of-the-art baselines. This work removes disturbance of non-contributing event types with the aid of a validation procedure, similar to the practice to mitigate overfitting used when training machine learning models.},
  eventtitle = {{{WWW}} '21: {{The Web Conference}} 2021},
  isbn = {978-1-4503-8312-7},
  langid = {english},
  annotation = {7 citations (Semantic Scholar/DOI) [2022-05-09]},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\RXM2FHY8\\Zhang et al. - 2021 - Learning Neural Point Processes with Latent Graphs.pdf}
}

@inproceedings{zhangSelfAttentiveHawkesProcess2020,
  title = {Self-{{Attentive Hawkes Process}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Qiang and Lipani, Aldo and Kirnap, Omer and Yilmaz, Emine},
  date = {2020-11-21},
  pages = {11183--11193},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/zhang20q.html},
  urldate = {2022-05-09},
  abstract = {Capturing the occurrence dynamics is crucial to predicting which type of events will happen next and when. A common method to do this is through Hawkes processes. To enhance their capacity, recurrent neural networks (RNNs) have been incorporated due to RNNs’ successes in processing sequential data such as languages. Recent evidence suggests that self-attention is more competent than RNNs in dealing with languages. However, we are unaware of the effectiveness of self-attention in the context of Hawkes processes. This study aims to fill the gap by designing a self-attentive Hawkes process (SAHP). SAHP employs self-attention to summarise the influence of history events and compute the probability of the next event. One deficit of the conventional self-attention when applied to event sequences is that its positional encoding only considers the order of a sequence ignoring the time intervals between events. To overcome this deficit, we modify its encoding by translating time intervals into phase shifts of sinusoidal functions. Experiments on goodness-of-fit and prediction tasks show the improved capability of SAHP. Furthermore, SAHP is more interpretable than RNN-based counterparts because the learnt attention weights reveal contributions of one event type to the happening of another type. To the best of our knowledge, this is the first work that studies the effectiveness of self-attention in Hawkes processes.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\CT5ZZ3EA\\Zhang et al. - 2020 - Self-Attentive Hawkes Process.pdf;C\:\\Users\\hokarami\\Zotero\\storage\\V2ZTGYAV\\Zhang et al_2020_Self-Attentive Hawkes Process.pdf}
}

@inproceedings{zuoTransformerHawkesProcess2020a,
  title = {Transformer {{Hawkes Process}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Zuo, Simiao and Jiang, Haoming and Li, Zichong and Zhao, Tuo and Zha, Hongyuan},
  date = {2020-11-21},
  pages = {11692--11702},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/zuo20a.html},
  urldate = {2022-05-06},
  abstract = {Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\hokarami\\Zotero\\storage\\FFEV6YWA\\Zuo et al. - 2020 - Transformer Hawkes Process.pdf;C\:\\Users\\hokarami\\Zotero\\storage\\K5LDG4WX\\Zuo et al_2020_Transformer Hawkes Process.pdf}
}
